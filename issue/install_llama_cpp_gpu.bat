@echo off
chcp 65001 >nul
REM Install llama-cpp-python with CUDA support after VS Build Tools installation
REM à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ llama-cpp-python à¸žà¸£à¹‰à¸­à¸¡ GPU support à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ VS Build Tools à¹à¸¥à¹‰à¸§

echo ðŸ”§ à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ llama-cpp-python à¸žà¸£à¹‰à¸­à¸¡ CUDA support...

REM à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ project directory
if not exist "rag_chatbot.py" (
    echo âŒ à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ rag_chatbot.py
    echo    à¸à¸£à¸¸à¸“à¸² cd à¹€à¸‚à¹‰à¸²à¹„à¸›à¹ƒà¸™ LLM-RAG directory à¸à¹ˆà¸­à¸™
    pause
    exit /b 1
)

REM à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š virtual environment
if not exist "llm_rag_env" (
    echo âŒ à¹„à¸¡à¹ˆà¸žà¸š virtual environment
    echo    à¸à¸£à¸¸à¸“à¸²à¸£à¸±à¸™ setup_python_env_windows.bat à¸à¹ˆà¸­à¸™
    pause
    exit /b 1
)

echo ðŸ”„ à¹€à¸›à¸´à¸”à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ virtual environment...
call llm_rag_env\Scripts\activate.bat

echo ðŸ“¦ à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Visual Studio Build Tools...
where cl >nul 2>&1
if errorlevel 1 (
    echo ðŸ” à¹„à¸¡à¹ˆà¸žà¸š Visual Studio Build Tools à¹ƒà¸™ PATH
    echo    à¸à¸³à¸¥à¸±à¸‡à¹€à¸žà¸´à¹ˆà¸¡ VS Build Tools à¹ƒà¸™ PATH...
    
    REM à¹€à¸žà¸´à¹ˆà¸¡ VS Build Tools à¹ƒà¸™ PATH
    call "C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat" 2>nul
    if errorlevel 1 (
        echo âŒ à¹„à¸¡à¹ˆà¸žà¸š Visual Studio Build Tools
        echo    à¸à¸£à¸¸à¸“à¸²à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Visual Studio Build Tools 2022 à¸à¹ˆà¸­à¸™
        echo    https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2022
        pause
        exit /b 1
    )
) else (
    echo âœ… à¸žà¸š Visual Studio Build Tools à¹à¸¥à¹‰à¸§
)

echo ðŸ”¥ à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ llama-cpp-python à¸žà¸£à¹‰à¸­à¸¡ CUDA support...

REM à¸¥à¸š version à¹€à¸à¹ˆà¸²à¸à¹ˆà¸­à¸™ (à¸–à¹‰à¸²à¸¡à¸µ)
pip uninstall llama-cpp-python -y

REM à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² environment variables à¸ªà¸³à¸«à¸£à¸±à¸š CUDA
set CMAKE_ARGS=-DLLAMA_CUBLAS=on
set FORCE_CMAKE=1

echo ðŸ“‹ CMAKE_ARGS: %CMAKE_ARGS%
echo ðŸ“‹ FORCE_CMAKE: %FORCE_CMAKE%

REM à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ llama-cpp-python
echo â¬‡ï¸ à¸à¸³à¸¥à¸±à¸‡à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡... (à¸­à¸²à¸ˆà¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸² 5-10 à¸™à¸²à¸—à¸µ)
pip install llama-cpp-python==0.2.90 --no-cache-dir --force-reinstall --verbose

if errorlevel 1 (
    echo âŒ à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹„à¸¡à¹ˆà¸ªà¸³à¹€à¸£à¹‡à¸ˆ!
    echo ðŸ’¡ à¸¥à¸­à¸‡à¸§à¸´à¸˜à¸µà¸­à¸·à¹ˆà¸™: à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸šà¸š CPU-only à¸à¹ˆà¸­à¸™
    echo.
    set /p choice="à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸šà¸š CPU-only à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ? (y/n): "
    if /i "%choice%"=="y" (
        echo ðŸ”„ à¸à¸³à¸¥à¸±à¸‡à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸šà¸š CPU-only...
        pip install llama-cpp-python==0.2.90 --no-cache-dir
        if not errorlevel 1 (
            echo âœ… à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸šà¸š CPU-only à¸ªà¸³à¹€à¸£à¹‡à¸ˆ
            echo âš ï¸ à¸ˆà¸°à¹ƒà¸Šà¹‰ CPU à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ (à¸Šà¹‰à¸²à¸à¸§à¹ˆà¸² GPU)
        )
    )
) else (
    echo âœ… à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ llama-cpp-python à¸ªà¸³à¹€à¸£à¹‡à¸ˆ!
    
    REM à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ CUDA
    echo ðŸ§ª à¸—à¸”à¸ªà¸­à¸š CUDA support...
    python -c "from llama_cpp import Llama; print('âœ… llama-cpp-python à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸ªà¸³à¹€à¸£à¹‡à¸ˆ')"
    
    if not errorlevel 1 (
        echo âœ… llama-cpp-python à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™!
        echo ðŸš€ à¸•à¸­à¸™à¸™à¸µà¹‰à¸ªà¸²à¸¡à¸²à¸£à¸–à¸£à¸±à¸™ application à¹„à¸”à¹‰à¹à¸¥à¹‰à¸§
    )
)

echo.
echo ðŸ“‹ à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸•à¹ˆà¸­à¹„à¸›:
echo 1. à¸£à¸±à¸™ application: streamlit run rag_chatbot.py
echo 2. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š log à¸§à¹ˆà¸²à¸¡à¸µà¸à¸²à¸£à¹ƒà¸Šà¹‰ GPU à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
echo    - à¸«à¸² "assigned to device CUDA0" à¹ƒà¸™ log
echo    - à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§ > 3000 tokens/sec = GPU working
echo    - à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§ ~2500 tokens/sec = CPU only
echo.

pause
