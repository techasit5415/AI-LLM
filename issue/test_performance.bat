@echo off
chcp 65001 >nul
REM Performance Test for llama-cpp-python installation
REM р╕Чр╕Фр╕кр╕нр╕Ър╕Ыр╕гр╕░р╕кр╕┤р╕Чр╕Шр╕┤р╕ар╕▓р╕Юр╕лр╕ер╕▒р╕Зр╕Бр╕▓р╕гр╕Хр╕┤р╕Фр╕Хр╕▒р╣Йр╕З llama-cpp-python

echo ЁЯЪА р╕Чр╕Фр╕кр╕нр╕Ър╕Ыр╕гр╕░р╕кр╕┤р╕Чр╕Шр╕┤р╕ар╕▓р╕Ю llama-cpp-python
echo =====================================

REM р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ър╕зр╣Ир╕▓р╕нр╕вр╕╣р╣Ир╣Гр╕Щ project directory
if not exist "rag_chatbot.py" (
    echo тЭМ р╣Др╕бр╣Ир╕Юр╕Ър╣Др╕Яр╕ер╣М rag_chatbot.py
    echo    р╕Бр╕гр╕╕р╕Ур╕▓ cd р╣Ар╕Вр╣Йр╕▓р╣Др╕Ыр╣Гр╕Щ LLM-RAG directory р╕Бр╣Ир╕нр╕Щ
    pause
    exit /b 1
)

REM р╣Ар╕Ыр╕┤р╕Фр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ virtual environment
if exist "llm_rag_env\Scripts\activate.bat" (
    echo ЁЯФД р╣Ар╕Ыр╕┤р╕Фр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ virtual environment...
    call llm_rag_env\Scripts\activate.bat
) else (
    echo тЪая╕П р╣Др╕бр╣Ир╕Юр╕Ъ virtual environment
)

echo ЁЯзк р╕Чр╕Фр╕кр╕нр╕Ър╕Бр╕▓р╕г import llama-cpp-python...
python -c "from llama_cpp import Llama; print('тЬЕ Import р╕кр╕│р╣Ар╕гр╣Зр╕И')" 2>nul
if errorlevel 1 (
    echo тЭМ р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Ц import llama-cpp-python р╣Др╕Фр╣Й
    echo ЁЯТб р╕Бр╕гр╕╕р╕Ур╕▓р╕Хр╕┤р╕Фр╕Хр╕▒р╣Йр╕З llama-cpp-python р╕Бр╣Ир╕нр╕Щ
    pause
    exit /b 1
)

echo ЁЯФН р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ version р╣Бр╕ер╕░ features...
python -c "
import llama_cpp
print(f'тЬЕ llama-cpp-python version: {llama_cpp.__version__}')

# р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ CUDA support
try:
    import llama_cpp.llama_cpp as llama_cpp_lib
    if hasattr(llama_cpp_lib, 'GGML_USE_CUBLAS'):
        print('ЁЯЪА CUDA support: Available')
    else:
        print('ЁЯТ╗ CUDA support: Not available (CPU-only)')
except:
    print('ЁЯТ╗ Mode: CPU-only')

# р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ GPU
import subprocess
try:
    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], 
                          capture_output=True, text=True, timeout=5)
    if result.returncode == 0:
        lines = result.stdout.strip().split('\n')
        for i, line in enumerate(lines):
            name, memory = line.split(', ')
            print(f'ЁЯОо GPU {i}: {name} ({memory} MB)')
    else:
        print('ЁЯТ╗ GPU: Not detected')
except:
    print('ЁЯТ╗ GPU: Not available')
"

echo.
echo ЁЯУК р╕Чр╕Фр╕кр╕нр╕Ъ performance р╕Юр╕╖р╣Йр╕Щр╕Рр╕▓р╕Щ...
echo    (р╕Чр╕Фр╕кр╕нр╕Ър╕Бр╕▓р╕гр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕ер╕Вр╕Щр╕▓р╕Фр╣Ар╕ер╣Зр╕Б - р╕Ир╕│р╕ер╕нр╕З)
echo.

python -c "
import time
from llama_cpp import Llama

print('ЁЯФД р╣Ар╕гр╕┤р╣Ир╕бр╕Чр╕Фр╕кр╕нр╕Ъ performance...')
start_time = time.time()

try:
    # р╕Ир╕│р╕ер╕нр╕Зр╕Бр╕▓р╕гр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕е (р╣Др╕бр╣Ир╣Др╕Фр╣Йр╣Вр╕лр╕ер╕Фр╕Ир╕гр╕┤р╕З)
    print('ЁЯУБ р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ LLM model path...')
    
    import os
    model_path = r'C:\AI\llm\Llama-3.2-3B-Instruct-GGUF\Llama-3.2-3B-Instruct-Q5_K_M.gguf'
    
    if os.path.exists(model_path):
        print(f'тЬЕ р╕Юр╕Ъ model file: {os.path.basename(model_path)}')
        file_size = os.path.getsize(model_path) / (1024**3)  # GB
        print(f'ЁЯУП р╕Вр╕Щр╕▓р╕Фр╣Др╕Яр╕ер╣М: {file_size:.2f} GB')
        
        # р╕Чр╕Фр╕кр╕нр╕Ър╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕ер╕Ир╕гр╕┤р╕З (р╣Гр╕Кр╣Й memory р╕Щр╣Йр╕нр╕в)
        print('ЁЯФД р╕Чр╕Фр╕кр╕нр╕Ър╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕е...')
        try:
            # р╣Гр╕Кр╣Й context_length р╣Ар╕ер╣Зр╕Бр╣Ар╕Юр╕╖р╣Ир╕нр╕Ыр╕гр╕░р╕лр╕вр╕▒р╕Ф memory
            llm = Llama(
                model_path=model_path,
                n_ctx=512,  # context length р╣Ар╕ер╣Зр╕Б
                n_threads=4,  # р╕Ир╕│р╕Бр╕▒р╕Ф threads
                verbose=False
            )
            
            load_time = time.time() - start_time
            print(f'тЬЕ р╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕ер╕кр╕│р╣Ар╕гр╣Зр╕И р╣Гр╕Кр╣Йр╣Ар╕зр╕ер╕▓: {load_time:.2f} р╕зр╕┤р╕Щр╕▓р╕Чр╕╡')
            
            # р╕Чр╕Фр╕кр╕нр╕Ъ generation
            print('ЁЯдЦ р╕Чр╕Фр╕кр╕нр╕Ъ text generation...')
            gen_start = time.time()
            
            output = llm('Hello', max_tokens=5, echo=False)
            
            gen_time = time.time() - gen_start
            tokens_per_sec = 5 / gen_time if gen_time > 0 else 0
            
            print(f'ЁЯУЭ Generation р╕Чр╕Фр╕кр╕нр╕Ъ: {output[\"choices\"][0][\"text\"].strip()}')
            print(f'тЪб р╕Др╕зр╕▓р╕бр╣Ар╕гр╣Зр╕з: {tokens_per_sec:.1f} tokens/sec (р╕Чр╕Фр╕кр╕нр╕Ър╣Ар╕ер╣Зр╕Б)')
            
        except Exception as e:
            print(f'тЪая╕П р╣Др╕бр╣Ир╕кр╕▓р╕бр╕▓р╕гр╕Цр╣Вр╕лр╕ер╕Фр╣Вр╕бр╣Ар╕Фр╕ер╣Др╕Фр╣Й: {str(e)}')
            print('ЁЯТб р╕нр╕▓р╕Ир╣Ар╕Ыр╣Зр╕Щр╣Ар╕Юр╕гр╕▓р╕░ memory р╣Др╕бр╣Ир╣Ар╕Юр╕╡р╕вр╕Зр╕Юр╕нр╕лр╕гр╕╖р╕н model file р╣Ар╕кр╕╡р╕вр╕лр╕▓р╕в')
    else:
        print(f'тЭМ р╣Др╕бр╣Ир╕Юр╕Ъ model file: {model_path}')
        print('ЁЯТб р╕Бр╕гр╕╕р╕Ур╕▓р╕Фр╕▓р╕зр╕Щр╣Мр╣Вр╕лр╕ер╕Ф model file р╕Бр╣Ир╕нр╕Щ')
        
except Exception as e:
    print(f'тЭМ Error р╣Гр╕Щр╕Бр╕▓р╕гр╕Чр╕Фр╕кр╕нр╕Ъ: {str(e)}')

total_time = time.time() - start_time
print(f'тП▒я╕П р╣Ар╕зр╕ер╕▓р╕Чр╕Фр╕кр╕нр╕Ър╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф: {total_time:.2f} р╕зр╕┤р╕Щр╕▓р╕Чр╕╡')
"

echo.
echo ЁЯУЛ р╕кр╕гр╕╕р╕Ыр╕Ьр╕ер╕Бр╕▓р╕гр╕Чр╕Фр╕кр╕нр╕Ъ:
echo =====================
echo.
echo ЁЯТ╛ Memory Usage:
python -c "
import psutil
import os

process = psutil.Process(os.getpid())
memory_info = process.memory_info()
memory_mb = memory_info.rss / 1024 / 1024

print(f'   - Current process: {memory_mb:.1f} MB')

# System memory
mem = psutil.virtual_memory()
print(f'   - System total: {mem.total / 1024**3:.1f} GB')
print(f'   - System available: {mem.available / 1024**3:.1f} GB')
print(f'   - System usage: {mem.percent:.1f}%')
"

echo.
echo ЁЯОп р╕Др╕│р╣Бр╕Щр╕░р╕Щр╕│:
echo.
echo тЬЕ р╕лр╕▓р╕Бр╕Бр╕▓р╕гр╕Чр╕Фр╕кр╕нр╕Ър╕Ьр╣Ир╕▓р╕Щр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф:
echo    - р╕Юр╕гр╣Йр╕нр╕бр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ LLM-RAG application
echo    - р╕гр╕▒р╕Щ: streamlit run rag_chatbot.py
echo.
echo тЪая╕П р╕лр╕▓р╕Бр╕бр╕╡р╕Ыр╕▒р╕Нр╕лр╕▓:
echo    - р╕Хр╕гр╕зр╕Ир╕кр╕нр╕Ъ memory usage
echo    - р╕ер╕Фр╕Вр╕Щр╕▓р╕Ф context length
echo    - р╣Гр╕Кр╣Йр╣Вр╕бр╣Ар╕Фр╕ер╕Вр╕Щр╕▓р╕Фр╣Ар╕ер╣Зр╕Бр╕Бр╕зр╣Ир╕▓
echo.
echo ЁЯЪА р╕лр╕▓р╕Бр╕Хр╣Йр╕нр╕Зр╕Бр╕▓р╕г performance р╕Фр╕╡р╕Бр╕зр╣Ир╕▓:
echo    - р╕Хр╕┤р╕Фр╕Хр╕▒р╣Йр╕Зр╣Бр╕Ър╕Ъ GPU support
echo    - р╣Ар╕Юр╕┤р╣Ир╕б RAM р╕лр╕гр╕╖р╕н VRAM
echo    - р╣Гр╕Кр╣Й SSD р╕кр╕│р╕лр╕гр╕▒р╕Ъ model files
echo.

pause
