@echo off
chcp 65001 >nul
REM ========================================================================
REM  LLama-CPP-Python Diagnostic and Repair Script
REM  ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ llama-cpp-python
REM ========================================================================

title LLama-CPP-Python Diagnostic Tool

echo.
echo üîß LLama-CPP-Python Diagnostic Tool
echo ===================================
echo.
echo üìã ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ llama-cpp-python
echo.

REM ==================== Basic System Check ====================

echo üîç [‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö] System Diagnostic...
echo ----------------------------------------

echo üíª ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö:
systeminfo | findstr /B /C:"OS Name" /C:"OS Version" /C:"System Type" /C:"Total Physical Memory"

echo.
echo üêç ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Python:
python --version 2>nul || echo ‚ùå Python not found
python -c "import sys; print(f'Python executable: {sys.executable}')" 2>nul
python -c "import sys; print(f'Python path: {sys.path[0]}')" 2>nul

echo.
echo üì¶ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• pip:
pip --version 2>nul || echo ‚ùå pip not found

echo.
echo üéÆ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GPU:
nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader,nounits 2>nul || echo ‚ùå NVIDIA GPU/Driver not found

REM ==================== Virtual Environment Check ====================

echo.
echo üîç [Virtual Environment] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Virtual Environment...
echo --------------------------------------------------------

if exist "llm_rag_env" (
    echo ‚úÖ ‡∏û‡∏ö virtual environment
    
    call llm_rag_env\Scripts\activate.bat
    echo üìç Active environment: %VIRTUAL_ENV%
    
    echo.
    echo üì¶ Packages ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß:
    pip list | findstr -i "llama torch streamlit langchain faiss sentence"
    
) else (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö virtual environment
    echo üí° ‡∏£‡∏±‡∏ô: python -m venv llm_rag_env
)

REM ==================== Import Testing ====================

echo.
echo üîç [Import Testing] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö imports...
echo -----------------------------------

echo üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö core imports...
python -c "
import sys
packages = [
    ('streamlit', 'streamlit'),
    ('torch', 'torch'), 
    ('langchain', 'langchain'),
    ('langchain_community.llms', 'LlamaCpp'),
    ('langchain_community.vectorstores', 'FAISS'),
    ('sentence_transformers', 'SentenceTransformer'),
    ('llama_cpp', 'Llama')
]

for module, component in packages:
    try:
        if component:
            exec(f'from {module} import {component}')
        else:
            exec(f'import {module}')
        print(f'‚úÖ {module}: OK')
    except ImportError as e:
        print(f'‚ùå {module}: IMPORT ERROR - {e}')
    except Exception as e:
        print(f'‚ö†Ô∏è {module}: WARNING - {e}')
"

REM ==================== llama-cpp-python Specific Tests ====================

echo.
echo üîç [LLama-CPP-Python] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ llama-cpp-python...
echo ----------------------------------------------------

echo ü¶ô ‡∏ó‡∏î‡∏™‡∏≠‡∏ö llama-cpp-python import ‡πÅ‡∏•‡∏∞ version...
python -c "
try:
    from llama_cpp import Llama
    import llama_cpp
    print(f'‚úÖ llama-cpp-python version: {llama_cpp.__version__}')
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á instance (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà model)
    try:
        # ‡πÉ‡∏ä‡πâ dummy model path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö constructor
        print('üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Llama constructor...')
        print('‚úÖ Llama class accessible')
    except Exception as e:
        print(f'‚ö†Ô∏è Llama constructor warning: {e}')
        
except ImportError as e:
    print(f'‚ùå llama-cpp-python import failed: {e}')
    print('üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: pip install llama-cpp-python')
except Exception as e:
    print(f'‚ö†Ô∏è llama-cpp-python unexpected error: {e}')
"

REM ==================== CUDA Testing ====================

echo.
echo üîç [CUDA Testing] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö CUDA support...
echo ---------------------------------------

echo üéÆ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö PyTorch CUDA...
python -c "
try:
    import torch
    print(f'PyTorch version: {torch.__version__}')
    print(f'CUDA available: {torch.cuda.is_available()}')
    
    if torch.cuda.is_available():
        print(f'CUDA version: {torch.version.cuda}')
        print(f'cuDNN version: {torch.backends.cudnn.version()}')
        print(f'Device count: {torch.cuda.device_count()}')
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f'GPU {i}: {props.name} ({props.total_memory // 1024**3} GB)')
    else:
        print('üîç CUDA ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô - ‡∏à‡∏∞‡πÉ‡∏ä‡πâ CPU')
        
except ImportError:
    print('‚ùå PyTorch ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á')
except Exception as e:
    print(f'‚ö†Ô∏è PyTorch error: {e}')
"

echo.
echo üéÆ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö NVIDIA driver...
nvidia-smi 2>nul && (
    echo ‚úÖ NVIDIA driver ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
) || (
    echo ‚ùå NVIDIA driver ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    echo üí° ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á NVIDIA driver ‡∏à‡∏≤‡∏Å: https://www.nvidia.com/drivers
)

REM ==================== File System Check ====================

echo.
echo üîç [File System] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå...
echo -------------------------------------------

echo üìÅ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö project structure...
if exist "rag_chatbot.py" (
    echo ‚úÖ rag_chatbot.py
) else (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö rag_chatbot.py
)

if exist "pages\backend\rag_functions.py" (
    echo ‚úÖ pages\backend\rag_functions.py
) else (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö pages\backend\rag_functions.py
)

if exist "vector store" (
    echo ‚úÖ vector store directory
    dir "vector store" /b
) else (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö vector store directory
)

echo.
echo üìÅ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö model paths...
if exist "C:\AI\llm\Llama-3.2-3B-Instruct-GGUF\Llama-3.2-3B-Instruct-Q5_K_M.gguf" (
    echo ‚úÖ LLM model ‡∏û‡∏ö‡∏ó‡∏µ‡πà C:\AI\llm\
    for %%f in ("C:\AI\llm\Llama-3.2-3B-Instruct-GGUF\Llama-3.2-3B-Instruct-Q5_K_M.gguf") do echo    Size: %%~zf bytes
) else (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö LLM model ‡∏ó‡∏µ‡πà C:\AI\llm\
    echo üí° ‡∏£‡∏±‡∏ô download_models_windows.bat ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
)

if exist "C:\AI\embedding-models\all-MiniLM-L6-v2" (
    echo ‚úÖ Embedding model ‡∏û‡∏ö‡∏ó‡∏µ‡πà C:\AI\embedding-models\
) else (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Embedding model ‡∏ó‡∏µ‡πà C:\AI\embedding-models\
    echo üí° ‡∏à‡∏∞‡πÉ‡∏ä‡πâ online embedding ‡πÅ‡∏ó‡∏ô
)

REM ==================== Configuration Check ====================

echo.
echo üîç [Configuration] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤...
echo --------------------------------------

echo üìù ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path configuration ‡πÉ‡∏ô rag_chatbot.py...
findstr /n "llm_model.*=" rag_chatbot.py 2>nul | findstr /v "text_input" || echo ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö llm_model configuration

echo.
echo üìù ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö import statements...
findstr /n "from llama_cpp import" pages\backend\rag_functions.py 2>nul || echo ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö llama_cpp import

REM ==================== Performance Test ====================

echo.
echo üîç [Performance] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö performance...
echo -----------------------------------

echo ‚è±Ô∏è ‡∏ó‡∏î‡∏™‡∏≠‡∏ö import speed...
powershell -Command "Measure-Command { python -c 'from llama_cpp import Llama' } | Select-Object TotalMilliseconds" 2>nul || echo ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏î‡∏™‡∏≠‡∏ö performance

REM ==================== Repair Suggestions ====================

echo.
echo üîß [Repair Suggestions] ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç...
echo -------------------------------------------

python -c "
issues = []
fixes = []

# Check llama-cpp-python
try:
    from llama_cpp import Llama
except ImportError:
    issues.append('‚ùå llama-cpp-python ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏´‡∏£‡∏∑‡∏≠ import ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ')
    fixes.append('üí° Fix: pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cpu')

# Check torch
try:
    import torch
    if not torch.cuda.is_available():
        issues.append('‚ö†Ô∏è CUDA ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏à‡∏∞‡πÉ‡∏ä‡πâ CPU')
        fixes.append('üí° Fix: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á CUDA driver ‡πÅ‡∏•‡∏∞ PyTorch CUDA version')
except ImportError:
    issues.append('‚ùå PyTorch ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á')
    fixes.append('üí° Fix: pip install torch torchvision torchaudio')

# Check other packages
packages = ['streamlit', 'langchain', 'sentence_transformers']
for pkg in packages:
    try:
        exec(f'import {pkg}')
    except ImportError:
        issues.append(f'‚ùå {pkg} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á')
        fixes.append(f'üí° Fix: pip install {pkg}')

if issues:
    print('üîß ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö:')
    for issue in issues:
        print(f'   {issue}')
    print()
    print('üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:')
    for fix in fixes:
        print(f'   {fix}')
else:
    print('‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏õ‡∏Å‡∏ï‡∏¥')
"

REM ==================== Quick Fixes ====================

echo.
echo üîß [Quick Fixes] ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏î‡πà‡∏ß‡∏ô...
echo --------------------------------

echo üí° ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
echo    1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python ‡πÉ‡∏´‡∏°‡πà (CPU)
echo    2. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python ‡πÉ‡∏´‡∏°‡πà (GPU)
echo    3. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç FAISS compatibility
echo    4. ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï virtual environment
echo    5. ‡∏Ç‡πâ‡∏≤‡∏°
echo.

set /p fix_choice="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (1-5): "

if "%fix_choice%"=="1" (
    echo üíª ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python CPU version...
    pip uninstall llama-cpp-python -y
    pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
    echo ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
    
) else if "%fix_choice%"=="2" (
    echo üéÆ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python GPU version...
    pip uninstall llama-cpp-python -y
    set CMAKE_ARGS=-DLLAMA_CUBLAS=on
    pip install llama-cpp-python --no-cache-dir
    echo ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
    
) else if "%fix_choice%"=="3" (
    echo üîß ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç FAISS compatibility...
    pip install --upgrade langchain==0.1.20 langchain-community==0.0.38
    echo ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
    
) else if "%fix_choice%"=="4" (
    echo üîÑ ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï virtual environment...
    set /p confirm="‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö virtual environment? (y/n): "
    if /i "%confirm%"=="y" (
        rmdir /s /q llm_rag_env
        python -m venv llm_rag_env
        call llm_rag_env\Scripts\activate.bat
        pip install -r requirements.txt
        echo ‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
    )
) else (
    echo üìù ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏î‡πà‡∏ß‡∏ô
)

echo.
echo üìã ‡∏™‡∏£‡∏∏‡∏õ Diagnostic:
echo ==================
echo    - ‡∏î‡∏π log ‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏±‡∏ç‡∏´‡∏≤
echo    - ‡πÉ‡∏ä‡πâ Quick Fixes ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
echo    - ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏î‡∏π LLAMA_CPP_PYTHON_TROUBLESHOOTING.md
echo.

pause
