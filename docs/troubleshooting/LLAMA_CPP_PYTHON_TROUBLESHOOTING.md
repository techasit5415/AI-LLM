# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ llama-cpp-python ‡∏ö‡∏ô Windows
## ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

### üîç ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢

#### 1. ImportError: Could not import llama-cpp-python library
```
ImportError: Could not import llama-cpp-python library. 
Please install the llama-cpp-python library to use this embedding model: 
pip install llama-cpp-python
```

#### 2. Build Error: CMAKE_C_COMPILER not set
```
CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage
CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage
```

#### 3. FAISS Compatibility Error
```
TypeError: FAISS.__init__() got an unexpected keyword argument 'allow_dangerous_deserialization'
```

---

## üõ†Ô∏è ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç

### ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å:
1. **‡∏Ç‡∏≤‡∏î Visual Studio Build Tools** - ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö compile C++ code
2. **‡∏Ç‡∏≤‡∏î C++ Compiler** - llama-cpp-python ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ MSVC compiler
3. **Version ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô** - langchain ‡πÅ‡∏•‡∏∞ faiss version conflicts
4. **CUDA Setup ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á** - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU support

---

## üöÄ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Step-by-Step

### Method 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio Community (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) üåü

#### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio Community
```cmd
REM ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ú‡πà‡∏≤‡∏ô winget
winget install Microsoft.VisualStudio.2022.Community

REM ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å
REM https://visualstudio.microsoft.com/vs/community/
```

#### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Workloads
‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:
- ‚úÖ **Desktop development with C++**
- ‚úÖ **MSVC v143 - VS 2022 C++ x64/x86 build tools**
- ‚úÖ **Windows 10/11 SDK**
- ‚úÖ **CMake tools for Visual Studio**

#### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó Command Prompt
```cmd
REM ‡∏õ‡∏¥‡∏î command prompt ‡πÄ‡∏Å‡πà‡∏≤
REM ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ environment variables update
```

#### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python
```cmd
REM ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô virtual environment
llm_rag_env\Scripts\activate.bat

REM ‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ VS environment ‡∏Å‡πà‡∏≠‡∏ô
call "C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat"

REM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CPU-only (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
pip install llama-cpp-python==0.2.90 --no-cache-dir --force-reinstall

REM ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU support
set CMAKE_ARGS=-DLLAMA_CUBLAS=on
set FORCE_CMAKE=1
pip install llama-cpp-python==0.2.90 --no-cache-dir --force-reinstall

REM ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
python -c "from llama_cpp import Llama; print('‚úÖ Success!')"
```

#### üö® ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Build Tools:
```cmd
REM ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤
.\issue\fix_buildtools_installation.bat
```

---

### Method 2: ‡πÉ‡∏ä‡πâ Pre-compiled Wheels üîß

#### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CPU-only:
```cmd
pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
```

#### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CUDA 12.1:
```cmd
pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

#### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CUDA 11.8:
```cmd
pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu118
```

---

### Method 3: ‡πÉ‡∏ä‡πâ Alternative Package üì¶

#### Option A: ‡πÉ‡∏ä‡πâ transformers ‡πÅ‡∏ó‡∏ô
```cmd
pip install transformers torch torchvision torchaudio
```

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô `pages/backend/rag_functions.py`:
```python
# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà LlamaCpp ‡∏î‡πâ‡∏ß‡∏¢ HuggingFacePipeline
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ‡∏™‡∏£‡πâ‡∏≤‡∏á pipeline
model_name = "microsoft/DialoGPT-medium"  # ‡∏´‡∏£‡∏∑‡∏≠ model ‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
llm = HuggingFacePipeline(pipeline=pipe)
```

#### Option B: ‡πÉ‡∏ä‡πâ Online API
```cmd
pip install openai langchain-openai
```

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ OpenAI API:
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    api_key="your_api_key_here"
)
```

---

### Method 4: ‡πÉ‡∏ä‡πâ WSL2 (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Advanced Users) üêß

#### ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á WSL2:
```cmd
wsl --install Ubuntu
```

#### ‡πÉ‡∏ô Ubuntu:
```bash
# ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï system
sudo apt update && sudo apt upgrade -y

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
sudo apt install -y python3-pip python3-venv build-essential cmake

# ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment
python3 -m venv llm_rag_env
source llm_rag_env/bin/activate

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python==0.2.90

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
pip install streamlit langchain langchain-community faiss-cpu sentence-transformers
```

---

## üß™ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ Troubleshooting

### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö llama-cpp-python:
```cmd
python scripts/testing/benchmark_llama_cpp.py
# ‡∏´‡∏£‡∏∑‡∏≠
python -c "from llama_cpp import Llama; print('‚úÖ llama-cpp-python ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ')"
```

### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö CUDA support:
```cmd
python scripts/testing/check_llama_cuda.py
# ‡∏´‡∏£‡∏∑‡∏≠
python scripts/testing/validate_gpu_setup.py
```

### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö GPU detection:
```cmd
python scripts/testing/test_gpu.py
```

### ‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å tests:
```cmd
# Windows
.\scripts\testing\run_test_scripts.bat

# Linux/macOS  
./scripts/testing/run_test_scripts.sh
```

### ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU usage:
```cmd
REM ‡∏£‡∏±‡∏ô nvidia-smi ‡∏Ç‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô LLM
nvidia-smi

REM ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏ô log:
REM "assigned to device CUDA0"
```

---

## üìä Performance Comparison

| Method | Speed (tokens/sec) | Setup Difficulty | GPU Support |
|--------|-------------------|------------------|-------------|
| Visual Studio + CUDA | 3500-4000+ | Hard | ‚úÖ Yes |
| Pre-compiled CUDA wheel | 3000-3500 | Medium | ‚úÖ Yes |
| Pre-compiled CPU wheel | 2000-2500 | Easy | ‚ùå No |
| Transformers | 1000-1500 | Easy | ‚ö†Ô∏è Limited |
| WSL2 + CUDA | 3500-4000+ | Hard | ‚úÖ Yes |

---

## üîß ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: "No module named '_ctypes'"
```cmd
REM ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual C++ Redistributable
winget install Microsoft.VCRedist.2015+.x64
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: "CUDA runtime error"
```cmd
REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA version
nvcc --version
nvidia-smi

REM ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï CUDA driver
REM ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å https://developer.nvidia.com/cuda-downloads
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: "Out of memory"
```python
# ‡∏•‡∏î model size ‡∏´‡∏£‡∏∑‡∏≠ context length
llm = LlamaCpp(
    model_path=llm_model,
    temperature=temperature,
    max_tokens=max_length,
    n_ctx=2048,  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 4096
    n_gpu_layers=20,  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å -1 (all layers)
    n_batch=256,  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 512
)
```

---

## üìã Checklist ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á

### Pre-requirements:
- [ ] Windows 10/11 (64-bit)
- [ ] Python 3.8+ 
- [ ] NVIDIA GPU + Driver (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU support)
- [ ] 16GB+ RAM
- [ ] 50GB+ disk space

### Installation Steps:
- [ ] ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio Community/Build Tools
- [ ] ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å C++ workload
- [ ] ‡∏£‡∏µ‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó command prompt
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment
- [ ] ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö import
- [ ] ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
- [ ] ‡∏£‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö application

### Verification:
- [ ] `python -c "from llama_cpp import Llama"` ‡πÑ‡∏°‡πà‡∏°‡∏µ error
- [ ] `nvidia-smi` ‡πÅ‡∏™‡∏î‡∏á GPU usage ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô
- [ ] LLM speed > 3000 tokens/sec (GPU) ‡∏´‡∏£‡∏∑‡∏≠ > 2000 tokens/sec (CPU)
- [ ] Application ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ error

---

## üìû Support ‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

### Official Documentation:
- [llama-cpp-python GitHub](https://github.com/abetlen/llama-cpp-python)
- [Visual Studio Build Tools](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2022)
- [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)

### Community Resources:
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA) - Reddit community
- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)

### Alternative Projects:
- [Ollama](https://ollama.ai/) - Easy LLM management
- [LM Studio](https://lmstudio.ai/) - GUI for local LLMs
- [Text Generation WebUI](https://github.com/oobabooga/text-generation-webui)

---

## üéØ ‡∏™‡∏£‡∏∏‡∏õ

‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python ‡∏ö‡∏ô Windows ‡∏≠‡∏≤‡∏à‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÑ‡∏î‡πâ performance ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

**‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å **Method 1** (Visual Studio Community) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡πÑ‡∏î‡πâ GPU support ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡∏´‡∏≤‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á **Method 2** (Pre-compiled wheels) ‡∏´‡∏£‡∏∑‡∏≠ **Method 4** (WSL2)

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production use ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ **Method 4** (WSL2) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ stable ‡πÅ‡∏•‡∏∞ performance ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
