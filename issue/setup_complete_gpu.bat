@echo off
setlocal enabledelayedexpansion
chcp 65001 >nul
REM Complete setup script for llama-cpp-python with GPU support
REM ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏ö‡∏ä‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö llama-cpp-python ‡∏û‡∏£‡πâ‡∏≠‡∏° GPU support

echo üöÄ Setup llama-cpp-python ‡∏û‡∏£‡πâ‡∏≠‡∏° GPU support ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows
echo =================================================================

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô project directory
cd /d "%~dp0.."
if not exist "rag_chatbot.py" (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå rag_chatbot.py
    echo    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ cd ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô LLM-RAG directory ‡∏Å‡πà‡∏≠‡∏ô
    pause
    exit /b 1
)

echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Prerequisites...

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Python
python --version >nul 2>&1
if errorlevel 1 (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Python
    echo    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python 3.8+ ‡∏à‡∏≤‡∏Å https://python.org
    pause
    exit /b 1
) else (
    for /f "tokens=2" %%v in ('python --version 2^>^&1') do echo ‚úÖ Python %%v
)

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Git
git --version >nul 2>&1
if errorlevel 1 (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Git
    echo    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Git ‡∏à‡∏≤‡∏Å https://git-scm.com
    pause
    exit /b 1
) else (
    for /f "tokens=3" %%v in ('git --version 2^>^&1') do echo ‚úÖ Git %%v
)

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NVIDIA GPU
nvidia-smi >nul 2>&1
if errorlevel 1 (
    echo ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö NVIDIA GPU ‡∏´‡∏£‡∏∑‡∏≠ driver
    echo    ‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only
    set GPU_SUPPORT=false
) else (
    echo ‚úÖ ‡∏û‡∏ö NVIDIA GPU
    nvidia-smi | findstr "CUDA Version"
    set GPU_SUPPORT=true
)

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Virtual Environment
if not exist "llm_rag_env" (
    echo üì¶ ‡∏™‡∏£‡πâ‡∏≤‡∏á Python virtual environment...
    python -m venv llm_rag_env
    if errorlevel 1 (
        echo ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment ‡πÑ‡∏î‡πâ
        pause
        exit /b 1
    )
) else (
    echo ‚úÖ ‡∏û‡∏ö virtual environment ‡πÅ‡∏•‡πâ‡∏ß
)

echo üîÑ ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô virtual environment...
call llm_rag_env\Scripts\activate.bat

REM ‡∏≠‡∏±‡∏û‡πÄ‡∏Å‡∏£‡∏î pip
echo üì¶ ‡∏≠‡∏±‡∏û‡πÄ‡∏Å‡∏£‡∏î pip...
python -m pip install --upgrade pip

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Visual Studio Build Tools
echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Visual Studio Build Tools...
where cl >nul 2>&1
if errorlevel 1 (
    echo ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Visual Studio Build Tools
    
    REM ‡∏•‡∏≠‡∏á‡∏´‡∏≤ VS Build Tools ‡πÉ‡∏ô location ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
    if exist "C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat" (
        echo üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î VS Build Tools environment...
        call "C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat"
    ) else (
        echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Visual Studio Build Tools
        echo.
        echo üìã ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio Build Tools 2022:
        echo    1. ‡∏£‡∏±‡∏ô: winget install Microsoft.VisualStudio.2022.BuildTools
        echo    2. ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å: https://visualstudio.microsoft.com/downloads/
        echo    3. ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "C++ build tools" workload
        echo.
        set /p choice="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Build Tools ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
        if /i "%choice%"=="y" (
            echo üîΩ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio Build Tools...
            winget install Microsoft.VisualStudio.2022.BuildTools --accept-source-agreements --accept-package-agreements
            echo ‚è∞ ‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            pause
            exit /b 0
        )
        
        echo üí° ‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only ‡πÅ‡∏ó‡∏ô
        set GPU_SUPPORT=false
    )
) else (
    echo ‚úÖ ‡∏û‡∏ö Visual Studio Build Tools ‡πÅ‡∏•‡πâ‡∏ß
)

REM ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
echo üì¶ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô...
pip install wheel setuptools

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ llama-cpp-python ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô...
python -c "import llama_cpp; print('‚úÖ ‡∏û‡∏ö llama-cpp-python version:', llama_cpp.__version__)" 2>nul
if not errorlevel 1 (
    echo.
    echo ‚úÖ ‡∏û‡∏ö llama-cpp-python ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
    set /p reinstall="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
    if /i "%reinstall%"=="n" (
        echo ‚è≠Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python
        goto :install_other_packages
    )
)

echo ü¶ô ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python...

REM ‡∏•‡∏ö version ‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô
echo üóëÔ∏è ‡∏•‡∏ö version ‡πÄ‡∏Å‡πà‡∏≤...
pip uninstall llama-cpp-python -y >nul 2>&1

if "%GPU_SUPPORT%"=="true" (
    echo.
    echo üöÄ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° CUDA support...
    echo ‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 5-15 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Visual Studio Build Tools
    echo.
    
    set /p gpu_choice="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
    if /i "%gpu_choice%"=="y" (
        echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA version...
        nvcc --version | findstr "release"
        
        echo ‚¨áÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CUDA 12.1...
        echo üìã ‡πÉ‡∏ä‡πâ pre-compiled CUDA wheel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
        pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.90-cu121/llama_cpp_python-0.2.90-cp311-cp311-win_amd64.whl --force-reinstall --no-deps
        
        if errorlevel 1 (
            echo.
            echo ‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à^!
            echo.
            echo üí° ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ:
            echo    - ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ (‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î 447MB)
            echo    - CUDA version ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
            echo    - Memory ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
            echo.
            echo üîÑ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ alternative installation methods...
            echo.
            echo üìù Method 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å index repository
            pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 --no-cache-dir --force-reinstall --no-deps
            
            if not errorlevel 1 (
                echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Method 1)^!
                goto :test_installation
            )
            
            echo.
            echo üìù Method 2: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU fallback
            set /p fallback="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡πÅ‡∏ó‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
            if /i "%fallback%"=="y" (
                echo üíª ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU...
                pip install llama-cpp-python --no-cache-dir
                if not errorlevel 1 (
                    echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à^!
                ) else (
                    echo ‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                    goto :installation_failed
                )
            ) else (
                echo ‚ùå ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
                goto :installation_failed
            )
        ) else (
            echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à^!
        )
    ) else (
        echo üíª ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å...
        pip install llama-cpp-python --no-cache-dir
        if not errorlevel 1 (
            echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à^!
        ) else (
            goto :installation_failed
        )
    )
) else (
    echo üíª ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only...
    pip install llama-cpp-python --no-cache-dir
    if not errorlevel 1 (
        echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à^!
    ) else (
        goto :installation_failed
    )
)

REM ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
echo üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python...
echo.

echo üìã ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö version ‡πÅ‡∏•‡∏∞ import...
python -c "import llama_cpp; print(f'‚úÖ llama-cpp-python version: {llama_cpp.__version__}')" 2>nul
if errorlevel 1 (
    echo ‚ùå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ import llama_cpp ‡πÑ‡∏î‡πâ
    echo üí° ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π error:
    echo    python -c "import llama_cpp"
    goto :installation_failed
)

echo.
echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA support...
python -c "import llama_cpp.llama_cpp as llama_cpp_lib; print('‚úÖ CUDA Support available') if hasattr(llama_cpp_lib, 'GGML_USE_CUDA') else print('‚ö†Ô∏è CPU-only mode')" 2>nul

if "%GPU_SUPPORT%"=="true" (
    echo.
    echo üíæ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU availability...
    python -c "import torch; print(f'‚úÖ GPU Available: {torch.cuda.is_available()}'); print(f'GPU Count: {torch.cuda.device_count()}'); print(f'GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')" 2>nul
    if errorlevel 1 (
        echo ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU ‡πÑ‡∏î‡πâ (PyTorch ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ)
    )
)

echo ‚úÖ llama-cpp-python ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô^!

:install_other_packages

REM ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
echo üì¶ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏≠‡∏∑‡πà‡∏ô‡πÜ...
pip install streamlit langchain langchain-community faiss-cpu sentence-transformers

echo.
echo üéâ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô^!
echo ===================
echo.
echo üìã ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:
echo 1. ‡∏£‡∏±‡∏ô application: streamlit run rag_chatbot.py
echo 2. ‡πÄ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: http://localhost:8501
echo 3. ‡∏Å‡∏î "Create chatbot" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
echo.
echo üî¨ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö performance (optional):
echo    python scripts\testing\test_gpu.py
echo.
echo üß™ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:
echo    python scripts\testing\validate_gpu_setup.py
echo.
echo üìä ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:
if "%GPU_SUPPORT%"=="true" (
    echo    - GPU mode: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß 25-35+ tokens/sec
    echo    - ‡∏î‡∏π log ‡∏´‡∏≤ "offloaded 29/29 layers to GPU"
    echo    - VRAM usage: ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2-3GB
    echo    - ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ CPU mode ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3 ‡πÄ‡∏ó‡πà‡∏≤
) else (
    echo    - CPU mode: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß 8-12 tokens/sec
    echo    - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ RAM: ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 3-4GB
)
echo.
echo üí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å CPU ‡πÄ‡∏õ‡πá‡∏ô GPU ‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
echo    ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà
echo.

pause
exit /b 0

:installation_failed
echo.
echo ‚ùå ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
echo ==================
echo.
echo üîß ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
echo.
echo 1Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞ retry:
echo    ‡πÑ‡∏ü‡∏•‡πå GPU wheel ‡∏Ç‡∏ô‡∏≤‡∏î 447MB ‡∏≠‡∏≤‡∏à‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
echo.
echo 2Ô∏è‚É£ ‡πÉ‡∏ä‡πâ alternative download method:
echo    pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 --no-cache-dir --no-deps
echo.
echo 3Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA version compatibility:
echo    nvcc --version (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô CUDA 12.1)
echo.
echo 4Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Visual Studio Build Tools:
echo    winget install Microsoft.VisualStudio.2022.BuildTools
echo.
echo 5Ô∏è‚É£ ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢:
echo    .\issue\diagnose_llama_cpp.bat
echo.
echo 6Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only ‡∏´‡∏≤‡∏Å‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:
echo    pip install llama-cpp-python --no-cache-dir
echo.
echo 7Ô∏è‚É£ ‡∏î‡∏π troubleshooting guide:
echo    .\issue\LLAMA_CPP_PYTHON_TROUBLESHOOTING.md
echo.
echo üí° ‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö: ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡πâ‡∏ß
echo    ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏à‡∏∞‡∏à‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ
echo.

pause
exit /b 1
