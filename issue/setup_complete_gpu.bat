@echo off
chcp 65001 >nul
REM Complete setup script for llama-cpp-python with GPU support
REM ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏ö‡∏ä‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö llama-cpp-python ‡∏û‡∏£‡πâ‡∏≠‡∏° GPU support

echo üöÄ Setup llama-cpp-python ‡∏û‡∏£‡πâ‡∏≠‡∏° GPU support ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Windows
echo =================================================================

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô project directory
cd /d "%~dp0.."
if not exist "rag_chatbot.py" (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå rag_chatbot.py
    echo    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ cd ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô LLM-RAG directory ‡∏Å‡πà‡∏≠‡∏ô
    pause
    exit /b 1
)

echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Prerequisites...

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Python
python --version >nul 2>&1
if errorlevel 1 (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Python
    echo    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python 3.8+ ‡∏à‡∏≤‡∏Å https://python.org
    pause
    exit /b 1
) else (
    for /f "tokens=2" %%v in ('python --version 2^>^&1') do echo ‚úÖ Python %%v
)

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Git
git --version >nul 2>&1
if errorlevel 1 (
    echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Git
    echo    ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Git ‡∏à‡∏≤‡∏Å https://git-scm.com
    pause
    exit /b 1
) else (
    for /f "tokens=3" %%v in ('git --version 2^>^&1') do echo ‚úÖ Git %%v
)

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NVIDIA GPU
nvidia-smi >nul 2>&1
if errorlevel 1 (
    echo ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö NVIDIA GPU ‡∏´‡∏£‡∏∑‡∏≠ driver
    echo    ‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only
    set GPU_SUPPORT=false
) else (
    echo ‚úÖ ‡∏û‡∏ö NVIDIA GPU
    nvidia-smi | findstr "CUDA Version"
    set GPU_SUPPORT=true
)

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Virtual Environment
if not exist "llm_rag_env" (
    echo üì¶ ‡∏™‡∏£‡πâ‡∏≤‡∏á Python virtual environment...
    python -m venv llm_rag_env
    if errorlevel 1 (
        echo ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á virtual environment ‡πÑ‡∏î‡πâ
        pause
        exit /b 1
    )
) else (
    echo ‚úÖ ‡∏û‡∏ö virtual environment ‡πÅ‡∏•‡πâ‡∏ß
)

echo üîÑ ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô virtual environment...
call llm_rag_env\Scripts\activate.bat

REM ‡∏≠‡∏±‡∏û‡πÄ‡∏Å‡∏£‡∏î pip
echo üì¶ ‡∏≠‡∏±‡∏û‡πÄ‡∏Å‡∏£‡∏î pip...
python -m pip install --upgrade pip

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Visual Studio Build Tools
echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Visual Studio Build Tools...
where cl >nul 2>&1
if errorlevel 1 (
    echo ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Visual Studio Build Tools
    
    REM ‡∏•‡∏≠‡∏á‡∏´‡∏≤ VS Build Tools ‡πÉ‡∏ô location ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
    if exist "C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat" (
        echo üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î VS Build Tools environment...
        call "C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat"
    ) else (
        echo ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Visual Studio Build Tools
        echo.
        echo üìã ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio Build Tools 2022:
        echo    1. ‡∏£‡∏±‡∏ô: winget install Microsoft.VisualStudio.2022.BuildTools
        echo    2. ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å: https://visualstudio.microsoft.com/downloads/
        echo    3. ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "C++ build tools" workload
        echo.
        set /p choice="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Build Tools ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
        if /i "%choice%"=="y" (
            echo üîΩ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Visual Studio Build Tools...
            winget install Microsoft.VisualStudio.2022.BuildTools --accept-source-agreements --accept-package-agreements
            echo ‚è∞ ‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            pause
            exit /b 0
        )
        
        echo üí° ‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only ‡πÅ‡∏ó‡∏ô
        set GPU_SUPPORT=false
    )
) else (
    echo ‚úÖ ‡∏û‡∏ö Visual Studio Build Tools ‡πÅ‡∏•‡πâ‡∏ß
)

REM ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
echo üì¶ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô...
pip install wheel setuptools

REM ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ llama-cpp-python ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô...
python -c "import llama_cpp; print('‚úÖ ‡∏û‡∏ö llama-cpp-python version:', llama_cpp.__version__)" 2>nul
if not errorlevel 1 (
    echo.
    echo ÔøΩ ‡∏û‡∏ö llama-cpp-python ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
    set /p reinstall="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
    if /i "%reinstall%"=="n" (
        echo ‚è≠Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python
        goto :install_other_packages
    )
)

echo ÔøΩü¶ô ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á llama-cpp-python...

REM ‡∏•‡∏ö version ‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô
echo üóëÔ∏è ‡∏•‡∏ö version ‡πÄ‡∏Å‡πà‡∏≤...
pip uninstall llama-cpp-python -y >nul 2>&1

if "%GPU_SUPPORT%"=="true" (
    echo.
    echo üöÄ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° CUDA support...
    echo ‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 5-15 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Visual Studio Build Tools
    echo.
    
    set /p gpu_choice="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
    if /i "%gpu_choice%"=="y" (
        echo üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö CUDA version...
        nvcc --version | findstr "release"
        
        echo ‚¨áÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CUDA 12.1...
        pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 --no-cache-dir --force-reinstall
        
        if errorlevel 1 (
            echo.
            echo ‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
            echo.
            echo üí° ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ:
            echo    - ‡πÑ‡∏°‡πà‡∏°‡∏µ Visual Studio Build Tools ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
            echo    - ‡πÑ‡∏°‡πà‡∏°‡∏µ CUDA Toolkit
            echo    - Memory ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
            echo.
            set /p fallback="‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡πÅ‡∏ó‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): "
            if /i "%fallback%"=="y" (
                echo üíª ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU...
                pip install llama-cpp-python --no-cache-dir
                if not errorlevel 1 (
                    echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
                ) else (
                    echo ‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                    goto :installation_failed
                )
            ) else (
                echo ‚ùå ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
                goto :installation_failed
            )
        ) else (
            echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö GPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
        )
    ) else (
        echo üíª ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å...
        pip install llama-cpp-python --no-cache-dir
        if not errorlevel 1 (
            echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
        ) else (
            goto :installation_failed
        )
    )
) else (
    echo üíª ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only...
    pip install llama-cpp-python --no-cache-dir
    if not errorlevel 1 (
        echo ‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!
    ) else (
        goto :installation_failed
    )
)

REM ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
echo üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö llama-cpp-python...
python -c "from llama_cpp import Llama; print('‚úÖ llama-cpp-python ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à')" 2>nul
if errorlevel 1 (
    echo ‚ùå ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
    echo üí° ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π error:
    echo    python -c "from llama_cpp import Llama"
    goto :installation_failed
) else (
    echo ‚úÖ llama-cpp-python ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!
)

:install_other_packages

REM ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
echo üì¶ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏≠‡∏∑‡πà‡∏ô‡πÜ...
pip install streamlit langchain langchain-community faiss-cpu sentence-transformers

echo.
echo üéâ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!
echo ===================
echo.
echo üìã ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:
echo 1. ‡∏£‡∏±‡∏ô application: streamlit run rag_chatbot.py
echo 2. ‡πÄ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: http://localhost:8501
echo 3. ‡∏Å‡∏î "Create chatbot" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
echo 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö performance:
if "%GPU_SUPPORT%"=="true" (
    echo    - GPU mode: ‡∏´‡∏≤ "assigned to device CUDA0" ‡πÉ‡∏ô log
    echo    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß: 3000+ tokens/sec
) else (
    echo    - CPU mode: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß ~2500 tokens/sec
)
echo.
echo üí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å CPU ‡πÄ‡∏õ‡πá‡∏ô GPU ‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
echo    ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà
echo.

pause
exit /b 0

:installation_failed
echo.
echo ‚ùå ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
echo ==================
echo.
echo üîß ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
echo.
echo 1Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Visual Studio Build Tools:
echo    winget install Microsoft.VisualStudio.2022.BuildTools
echo.
echo 2Ô∏è‚É£ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ pre-compiled wheel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CUDA 12.1:
echo    pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 --no-cache-dir
echo.
echo 3Ô∏è‚É£ ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢:
echo    .\issue\diagnose_llama_cpp.bat
echo.
echo 4Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö CPU-only:
echo    pip install llama-cpp-python --no-cache-dir
echo.
echo 5Ô∏è‚É£ ‡∏î‡∏π troubleshooting guide:
echo    .\issue\LLAMA_CPP_PYTHON_TROUBLESHOOTING.md
echo.
echo üí° ‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö: ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡πâ‡∏ß
echo    ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏à‡∏∞‡∏à‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ
echo.

pause
exit /b 1
