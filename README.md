# LLM-RAG Chatbot
## Local LLM-powered RAG Chatbot with GPU Acceleration

‡∏£‡∏∞‡∏ö‡∏ö RAG (Retrieval-Augmented Generation) Chatbot ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Local LLM ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á HuggingFace API ‡∏´‡∏£‡∏∑‡∏≠ Cloud services ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Linux ‡πÅ‡∏•‡∏∞ Windows ‡∏û‡∏£‡πâ‡∏≠‡∏° GPU acceleration

### ‚ú® Features
- ü§ñ **Local LLM:** Llama 3.2 3B Instruct (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï)
- üöÄ **GPU Acceleration:** ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö NVIDIA CUDA
- üìö **RAG System:** ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
- üîç **Vector Search:** FAISS vector database
- üíª **Cross-platform:** Linux ‡πÅ‡∏•‡∏∞ Windows
- üåê **Web Interface:** Streamlit GUI

# LLM-RAG Chatbot
## Local LLM-powered RAG Chatbot with GPU Acceleration

‡∏£‡∏∞‡∏ö‡∏ö RAG (Retrieval-Augmented Generation) Chatbot ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Local LLM ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á HuggingFace API ‡∏´‡∏£‡∏∑‡∏≠ Cloud services ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Linux ‡πÅ‡∏•‡∏∞ Windows ‡∏û‡∏£‡πâ‡∏≠‡∏° GPU acceleration

### ‚ú® Features
- ü§ñ **Local LLM:** Llama 3.2 3B Instruct (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï)
- üöÄ **GPU Acceleration:** ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö NVIDIA CUDA
- üìö **RAG System:** ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
- üîç **Vector Search:** FAISS vector database
- üíª **Cross-platform:** Linux ‡πÅ‡∏•‡∏∞ Windows
- üåê **Web Interface:** Streamlit GUI

### üìä Performance
- **GPU mode:** 25-35+ tokens/sec (RTX 2060)
- **CPU mode:** 8-12 tokens/sec

---

##  Project Structure
```
AI-LLM/
‚îú‚îÄ‚îÄ rag_chatbot.py              # Main Streamlit application
‚îú‚îÄ‚îÄ rag_chatbot_windows.py      # Windows-specific version
‚îú‚îÄ‚îÄ pages/                      # Streamlit pages
‚îÇ   ‚îú‚îÄ‚îÄ document_embedding.py   # Document processing
‚îÇ   ‚îî‚îÄ‚îÄ backend/
‚îÇ       ‚îî‚îÄ‚îÄ rag_functions.py    # Core RAG logic
‚îú‚îÄ‚îÄ data/                       # üÜï Data directory
‚îÇ   ‚îú‚îÄ‚îÄ sources/                # Source documents (PDF, TXT)
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/             # Vector stores (FAISS)
‚îú‚îÄ‚îÄ scripts/                    # üÜï Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ testing/                # Testing & validation scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_gpu.py         # GPU performance testing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_gpu_setup.py # Setup validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ check_llama_cuda.py # CUDA compatibility check
‚îÇ   ‚îî‚îÄ‚îÄ utilities/              # Utility scripts
‚îÇ       ‚îú‚îÄ‚îÄ build_faiss_index.py # Vector store builder
‚îÇ       ‚îî‚îÄ‚îÄ fix_faiss_compatibility.py # FAISS API fixes
‚îú‚îÄ‚îÄ docs/                       # üÜï Documentation
‚îÇ   ‚îú‚îÄ‚îÄ DOCUMENTATION_INDEX.md  # Documentation index
‚îÇ   ‚îú‚îÄ‚îÄ QUICK_REFERENCE.md      # Quick reference guide
‚îÇ   ‚îî‚îÄ‚îÄ troubleshooting/        # Troubleshooting guides
‚îÇ       ‚îú‚îÄ‚îÄ LLAMA_CPP_PYTHON_TROUBLESHOOTING.md
‚îÇ       ‚îî‚îÄ‚îÄ GPU_SETUP_UPDATES.md
‚îú‚îÄ‚îÄ setup/                      # Installation scripts
‚îÇ   ‚îú‚îÄ‚îÄ linux/                  # Linux setup files
‚îÇ   ‚îî‚îÄ‚îÄ windows/                # Windows setup files
‚îú‚îÄ‚îÄ issue/                      # Troubleshooting scripts
‚îÇ   ‚îî‚îÄ‚îÄ setup_complete_gpu.bat  # GPU setup script
‚îî‚îÄ‚îÄ experiment/                 # Jupyter notebooks
    ‚îú‚îÄ‚îÄ LLM RAG experiment.ipynb
    ‚îî‚îÄ‚îÄ Run_Streamlit_of_LLM_RAG.ipynb
```

---

## üêß Linux Installation

### One-Command Install:
```bash
cd LLM-RAG
setup/linux/auto_setup.sh && source ~/.bashrc && setup/linux/setup_python_env.sh && setup/linux/download_models.sh
```

### Step-by-Step:
```bash
cd LLM-RAG

# 1. Install system dependencies (CUDA, Python, etc.)
setup/linux/auto_setup.sh
source ~/.bashrc

# 2. Setup Python environment and packages
setup/linux/setup_python_env.sh

# 3. Download LLM models (~2.16 GB) to ~/Documents/AI/
setup/linux/download_models.sh

# 4. Run application
setup/linux/run_app.sh
```

**LLM path:** `~/Documents/AI/llm/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q5_K_M.gguf`

### üìñ **Detailed Guide:** `setup/linux/SETUP_GUIDE.md`
### ‚ö° **Quick Reference:** `setup/linux/QUICK_START.md`

---

## üñ•Ô∏è Windows Installation

### üöÄ Quick GPU Setup (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥):
```cmd
cd AI-LLM
.\issue\setup_complete_gpu.bat
```

### üéØ Alternative Installation Menu:
```cmd
cd AI-LLM
.\issue\install_menu.bat
```

### Prerequisites:
1. **Python 3.11+** (from python.org)
2. **Git for Windows** (from git-scm.com)
3. **NVIDIA CUDA Toolkit 12.1** (for GPU support)
4. **Visual Studio Build Tools 2022** (for compiling C++ packages)

### Step-by-Step Manual Installation:
```cmd
cd AI-LLM

REM 1. Setup Python environment
setup\windows\setup_python_env_windows.bat

REM 2. Download LLM models (~2.16 GB) to C:\AI\
setup\windows\download_models_windows.bat

REM 3. Run application
setup\windows\run_app_windows.bat
```

**LLM path:** `C:\AI\llm\Llama-3.2-3B-Instruct-GGUF\Llama-3.2-3B-Instruct-Q5_K_M.gguf`

### üîß Testing & Validation:
```cmd
REM Test GPU performance
python scripts\testing\test_gpu.py

REM Validate installation
python scripts\testing\validate_gpu_setup.py

REM Check CUDA compatibility
python scripts\testing\check_llama_cuda.py
```

### üõ†Ô∏è Troubleshooting:
- **Documentation:** `docs\troubleshooting\`
- **Quick fixes:** `issue\diagnose_llama_cpp.bat`
- **GPU issues:** `docs\troubleshooting\GPU_SETUP_UPDATES.md`

### üìñ **Detailed Guide:** `setup/windows/WINDOWS_SETUP.md`
### ‚ö° **Quick Reference:** `setup/windows/WINDOWS_QUICK_START.md`

---

## üéØ Usage

1. **‡πÄ‡∏õ‡∏¥‡∏î browser** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `http://localhost:8501`
2. **‡∏Å‡∏î "Create chatbot"** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
3. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Vector Store** (naruto, snake, ‡∏´‡∏£‡∏∑‡∏≠ naruto_snake)
4. **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°!**

### Example Questions:
- "What is Naruto's main goal?" (‡πÉ‡∏ä‡πâ naruto vector store)
- "How do snakes shed their skin?" (‡πÉ‡∏ä‡πâ snake vector store)
- "Compare Naruto and snake characteristics" (‡πÉ‡∏ä‡πâ naruto_snake vector store)

### üìä Performance Monitoring:
- **GPU working:** ‡∏î‡∏π log ‡∏´‡∏≤ "offloaded 29/29 layers to GPU"
- **Speed > 25 tokens/sec:** GPU acceleration active
- **Speed ~10 tokens/sec:** CPU only

### üîß Utilities Available:
```cmd
REM Rebuild vector stores
python scripts\utilities\build_faiss_index.py

REM Fix FAISS compatibility issues  
python scripts\utilities\fix_faiss_compatibility.py
```

---

## üîß Troubleshooting

### Common Issues:

#### Linux:
```bash
# CUDA not working
export CUDA_HOME=/usr/local/cuda
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# llama-cpp-python issues
pip uninstall llama-cpp-python
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --no-cache-dir
```

#### Windows:
```cmd
REM CUDA not detected
set CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1
set PATH=%CUDA_PATH%\bin;%PATH%

REM PowerShell execution policy
powershell Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

##  License

This project uses various open-source components:
- **Llama 3.2:** Meta's Llama 3.2 license
- **LangChain:** MIT License
- **Streamlit:** Apache License 2.0

---

## üôè Acknowledgments

- **Meta AI** for Llama 3.2 model
- **LangChain** for RAG framework
- **Streamlit** for web interface
- **llama.cpp** community for GGUF format and CUDA support

---

## üìû Support

### Need Help?
1. **Read the documentation** in `docs/` directory
2. **Check troubleshooting guides** in `docs/troubleshooting/`
3. **Run validation scripts** in `scripts/testing/`
4. **Verify system requirements** are met

### Key Documentation:
- **Windows Setup:** `setup/windows/WINDOWS_SETUP.md`
- **Linux Setup:** `setup/linux/SETUP_GUIDE.md`
- **Quick Reference:** `docs/QUICK_REFERENCE.md`
- **GPU Troubleshooting:** `docs/troubleshooting/`

### üß™ Testing & Validation:
- **GPU Test:** `python scripts/testing/test_gpu.py`
- **Full Validation:** `python scripts/testing/validate_gpu_setup.py`
- **CUDA Check:** `python scripts/testing/check_llama_cuda.py`

---

This repository demonstrates the development of Retrieval-Augmented Generation (RAG). 
RAG enables LLM to generate answer text based on custom documents.
This repo also presents the RAG as a chatbot in Streamlit app.

Below is the diagram of the development:

![RAG Architecture Diagram](diagram.png)

# Interface Display
![Streamlit Interface](interface.png)

## üìù Related Article 
The article discussing the process of developing this application:

[RAG and Streamlit Chatbot: Chat with Documents Using LLM](https://www.analyticsvidhya.com/blog/2024/04/rag-and-streamlit-chatbot-chat-with-documents-using-llm/)
