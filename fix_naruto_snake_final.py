#!/usr/bin/env python3
"""
‡∏™‡∏£‡πâ‡∏≤‡∏á vector store ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö naruto_snake ‡∏î‡πâ‡∏ß‡∏¢ compatibility ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
"""

import os
import sys
sys.path.append('/home/techasit/Documents/Test/AI-LLM')

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
import hashlib

class CompatibleEmbeddings(Embeddings):
    """Compatible embeddings ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö FAISS"""
    def embed_documents(self, texts):
        embeddings = []
        for text in texts:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å hash
            hash_obj = hashlib.md5(str(text).encode('utf-8'))
            hash_hex = hash_obj.hexdigest()
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector ‡∏Ç‡∏ô‡∏≤‡∏î 128 dimensions
            embedding = []
            for i in range(0, 32, 2):
                embedding.append(int(hash_hex[i:i+2], 16) / 255.0)
            # ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô 128 dimensions
            embedding = embedding * 8
            embeddings.append(embedding)
        return embeddings
    
    def embed_query(self, text):
        return self.embed_documents([text])[0]

def main():
    try:
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á
        files = [
            "/home/techasit/Documents/Test/AI-LLM/data/sources/wikipedia_naruto.txt",
            "/home/techasit/Documents/Test/AI-LLM/data/sources/wikipedia_snake.txt"
        ]
        
        all_document = ""
        for file_path in files:
            print(f"Reading file: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                all_document += content + "\n\n"
        
        print(f"Combined document length: {len(all_document)} characters")
        
        # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô chunks ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=120,    # ‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÜ
            chunk_overlap=15   # overlap ‡∏ô‡πâ‡∏≠‡∏¢
        )
        splits = splitter.split_text(all_document)
        splits = splitter.create_documents(splits)
        
        print(f"Number of chunks: {len(splits)}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        for i in range(min(3, len(splits))):
            print(f"Chunk {i+1}: {splits[i].page_content[:60]}...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
        embeddings = CompatibleEmbeddings()
        print("Testing embeddings...")
        test_embed = embeddings.embed_query("test")
        print(f"Embedding dimension: {len(test_embed)}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS vector store
        print("Creating FAISS vector store...")
        db = FAISS.from_documents(splits, embeddings)
        
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤
        output_path = "/home/techasit/Documents/Test/AI-LLM/data/embeddings/vector store/naruto_snake"
        os.makedirs(output_path, exist_ok=True)
        
        for fname in ["index.faiss", "index.pkl"]:
            fpath = os.path.join(output_path, fname)
            if os.path.exists(fpath):
                os.remove(fpath)
                print(f"Removed old file: {fname}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å vector store ‡πÉ‡∏´‡∏°‡πà
        db.save_local(output_path)
        print(f"New naruto_snake vector store saved to: {output_path}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå
        faiss_size = os.path.getsize(os.path.join(output_path, "index.faiss"))
        pkl_size = os.path.getsize(os.path.join(output_path, "index.pkl"))
        print(f"File sizes: index.faiss = {faiss_size} bytes, index.pkl = {pkl_size} bytes")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        print("\nTesting load and search...")
        test_embeddings = CompatibleEmbeddings()
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î
        try:
            test_db = FAISS.load_local(output_path, test_embeddings)
            print("‚úÖ Successfully loaded vector store")
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Naruto
            retriever = test_db.as_retriever(search_kwargs={"k": 2})
            results1 = retriever.get_relevant_documents("What is Naruto?")
            print(f"‚úÖ Naruto search successful: {len(results1)} results")
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Snake
            results2 = retriever.get_relevant_documents("What are snakes?")
            print(f"‚úÖ Snake search successful: {len(results2)} results")
                
        except Exception as e:
            print(f"‚ùå Test failed: {e}")
            return False
        
        print("\nüéâ Naruto_Snake vector store successfully created and tested!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
