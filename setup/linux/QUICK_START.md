# Quick Start Guide
# ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà

## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (Full Setup)

### 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á System Dependencies
```bash
./auto_setup.sh
source ~/.bashrc
```

### 2. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python Environment  
```bash
cd LLM-RAG
./setup_python_env.sh
```

### 3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Models
```bash
./download_models.sh
```

### 4. ‡∏õ‡∏£‡∏±‡∏ö Path ‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå `rag_chatbot.py` ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î 46:
```python
# LLM path for Linux: 
llm_model = "/home/YOUR_USERNAME/Documents/AI/llm/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q5_K_M.gguf"

# Embedding path:
local_embed_path = "/home/YOUR_USERNAME/Documents/AI/embedding-models/all-MiniLM-L6-v2"
```

### 5. ‡∏£‡∏±‡∏ô Application
```bash
source llm_rag_env/bin/activate
streamlit run rag_chatbot.py
```

---

## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏î‡πà‡∏ß‡∏ô (Quick Setup)

### ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ CUDA ‡πÅ‡∏•‡πâ‡∏ß:
```bash
# 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
sudo apt update
sudo apt install -y python3-venv python3-pip build-essential cmake

# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á environment
python3 -m venv llm_rag_env
source llm_rag_env/bin/activate

# 3. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages
pip install -r requirements.txt
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python==0.2.90

# 4. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model
./download_models.sh

# 5. ‡∏£‡∏±‡∏ô
streamlit run rag_chatbot.py
```

---

## Files ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á Copy ‡πÑ‡∏õ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà

### Required Files:
```
LLM-RAG/
‚îú‚îÄ‚îÄ rag_chatbot.py              # Main application
‚îú‚îÄ‚îÄ pages/backend/rag_functions.py  # Backend functions
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ vector store/              # Pre-built vector databases
‚îÇ   ‚îú‚îÄ‚îÄ naruto/
‚îÇ   ‚îú‚îÄ‚îÄ snake/ 
‚îÇ   ‚îî‚îÄ‚îÄ naruto_snake/
‚îî‚îÄ‚îÄ data sources/              # Source documents
    ‚îú‚îÄ‚îÄ wikipedia_naruto.txt
    ‚îî‚îÄ‚îÄ wikipedia_snake.txt
```

### Setup Scripts:
```
‚îú‚îÄ‚îÄ auto_setup.sh              # ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á system dependencies
‚îú‚îÄ‚îÄ setup_python_env.sh        # ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python environment
‚îú‚îÄ‚îÄ download_models.sh          # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î models
‚îú‚îÄ‚îÄ install_system_deps.sh      # Manual system install
‚îú‚îÄ‚îÄ SETUP_GUIDE.md             # ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÄ‡∏ï‡πá‡∏°
‚îî‚îÄ‚îÄ QUICK_START.md             # ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
```

---

## Troubleshooting

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ CUDA:
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
nvidia-smi
nvcc --version

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç environment
export CUDA_HOME=/usr/local/cuda
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ llama-cpp-python:
```bash
pip uninstall llama-cpp-python
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --no-cache-dir
```

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Streamlit:
```bash
pip install --upgrade streamlit
```

---

## Performance Check

### ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ GPU:
- ‡∏î‡∏π‡πÉ‡∏ô log ‡∏ß‡πà‡∏≤‡∏°‡∏µ "assigned to device CUDA0" 
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß > 3000 tokens/second = ‡πÉ‡∏ä‡πâ GPU
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß ~2700 tokens/second = ‡πÉ‡∏ä‡πâ CPU

### Expected Performance:
- **RTX 4060:** ~4000 tokens/second
- **RTX 3060:** ~3500 tokens/second  
- **GTX 1660:** ~3000 tokens/second
- **CPU only:** ~2700 tokens/second

---

## üìÅ Model Paths ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Linux

### LLM Model:
```
Path: ~/Documents/AI/llm/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q5_K_M.gguf
Full Path: /home/$(whoami)/Documents/AI/llm/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q5_K_M.gguf
Size: ~2.16 GB
```

### Embedding Model (Optional):
```
Path: ~/Documents/AI/embedding-models/all-MiniLM-L6-v2
Full Path: /home/$(whoami)/Documents/AI/embedding-models/all-MiniLM-L6-v2
Size: ~90 MB
```
