from llama_cpp import Llama
import os

print("=== Testing llama-cpp-python GPU Support ===")

# Test if CUDA is available
try:
    import subprocess
    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
    if result.returncode == 0:
        print("‚úÖ NVIDIA GPU detected")
    else:
        print("‚ùå No NVIDIA GPU detected")
except:
    print("‚ùå nvidia-smi not available")

# Test loading model with GPU
model_path = r"C:\AI\llm\Llama-3.2-3B-Instruct-GGUF\Llama-3.2-3B-Instruct-Q5_K_M.gguf"
if os.path.exists(model_path):
    print(f"‚úÖ Model file found: {model_path}")
    
    try:
        print("üîÑ Loading model with GPU support (n_gpu_layers=-1)...")
        llm = Llama(
            model_path=model_path,
            n_gpu_layers=-1,  # Use all GPU layers
            verbose=True      # Show detailed logs
        )
        print("‚úÖ Model loaded successfully!")
        
        # Test inference
        print("üß™ Testing inference...")
        response = llm("Hello", max_tokens=10)
        print(f"Response: {response}")
        
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        
        # Try CPU-only mode
        print("üîÑ Trying CPU-only mode...")
        try:
            llm = Llama(
                model_path=model_path,
                n_gpu_layers=0,  # CPU only
                verbose=True
            )
            print("‚úÖ Model loaded in CPU mode")
        except Exception as e2:
            print(f"‚ùå CPU mode also failed: {e2}")
else:
    print(f"‚ùå Model file not found: {model_path}")
