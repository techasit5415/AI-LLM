#!/usr/bin/env python3
"""
‡∏™‡∏£‡πâ‡∏≤‡∏á vector store ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö naruto ‡∏î‡πâ‡∏ß‡∏¢ compatibility ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
"""

import os
import sys

# Get the project root directory (2 levels up from this script)
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(script_dir))
sys.path.append(project_root)

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
import hashlib

class CompatibleEmbeddings(Embeddings):
    """Compatible embeddings ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö FAISS"""
    def embed_documents(self, texts):
        embeddings = []
        for text in texts:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å hash
            hash_obj = hashlib.md5(str(text).encode('utf-8'))
            hash_hex = hash_obj.hexdigest()
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector ‡∏Ç‡∏ô‡∏≤‡∏î 128 dimensions
            embedding = []
            for i in range(0, 32, 2):
                embedding.append(int(hash_hex[i:i+2], 16) / 255.0)
            # ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô 128 dimensions
            embedding = embedding * 8
            embeddings.append(embedding)
        return embeddings
    
    def embed_query(self, text):
        return self.embed_documents([text])[0]

def main():
    try:
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå naruto data - ‡πÉ‡∏ä‡πâ path ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô cross-platform
        naruto_file = os.path.join(project_root, "data", "sources", "wikipedia_naruto.txt")
        print(f"Reading file: {naruto_file}")
        
        with open(naruto_file, 'r', encoding='utf-8') as f:
            document = f.read()
        
        print(f"Document length: {len(document)} characters")
        
        # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô chunks ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=120,    # ‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô snake
            chunk_overlap=15   # overlap ‡∏ô‡πâ‡∏≠‡∏¢
        )
        splits = splitter.split_text(document)
        splits = splitter.create_documents(splits)
        
        print(f"Number of chunks: {len(splits)}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        for i in range(min(3, len(splits))):
            print(f"Chunk {i+1}: {splits[i].page_content[:60]}...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
        embeddings = CompatibleEmbeddings()
        print("Testing embeddings...")
        test_embed = embeddings.embed_query("test")
        print(f"Embedding dimension: {len(test_embed)}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS vector store
        print("Creating FAISS vector store...")
        db = FAISS.from_documents(splits, embeddings)
        
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤
        output_path = os.path.join(project_root, "data", "embeddings", "vector store", "naruto")
        os.makedirs(output_path, exist_ok=True)
        
        for fname in ["index.faiss", "index.pkl"]:
            fpath = os.path.join(output_path, fname)
            if os.path.exists(fpath):
                os.remove(fpath)
                print(f"Removed old file: {fname}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å vector store ‡πÉ‡∏´‡∏°‡πà
        db.save_local(output_path)
        print(f"New naruto vector store saved to: {output_path}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå
        faiss_size = os.path.getsize(os.path.join(output_path, "index.faiss"))
        pkl_size = os.path.getsize(os.path.join(output_path, "index.pkl"))
        print(f"File sizes: index.faiss = {faiss_size} bytes, index.pkl = {pkl_size} bytes")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        print("\nTesting load and search...")
        test_embeddings = CompatibleEmbeddings()
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏•‡∏î
        try:
            test_db = FAISS.load_local(output_path, test_embeddings, allow_dangerous_deserialization=True)
            print("‚úÖ Successfully loaded vector store")
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            retriever = test_db.as_retriever(search_kwargs={"k": 2})
            results = retriever.get_relevant_documents("What is Naruto?")
            
            print(f"‚úÖ Search successful: {len(results)} results")
            for i, doc in enumerate(results):
                print(f"  Result {i+1}: {doc.page_content[:50]}...")
                
        except Exception as e:
            print(f"‚ùå Test failed: {e}")
            return False
        
        print("\nüéâ Naruto vector store successfully created and tested!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
